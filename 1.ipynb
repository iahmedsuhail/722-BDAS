{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data.\n",
    "df = spark.read.csv('dataset/ri_statewide_2019_02_25.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+----+------------+-----------+-------------+---------+-----------+---------------+--------------+--------+----------------+----------------+------------------+------------------+----------------+---------------+----------------+------------+-----------------+--------------------+------------+-------------+\n",
      "|raw_row_number|      date|    time|zone|subject_race|subject_sex|department_id|     type|arrest_made|citation_issued|warning_issued| outcome|contraband_found|contraband_drugs|contraband_weapons|contraband_alcohol|contraband_other|frisk_performed|search_conducted|search_basis|reason_for_search|     reason_for_stop|vehicle_make|vehicle_model|\n",
      "+--------------+----------+--------+----+------------+-----------+-------------+---------+-----------+---------------+--------------+--------+----------------+----------------+------------------+------------------+----------------+---------------+----------------+------------+-----------------+--------------------+------------+-------------+\n",
      "|             1|2005-11-22|11:15:00|  X3|       white|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|             2|2005-10-01|12:20:00|  X3|       white|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|             3|2005-10-01|12:30:00|  X3|       white|     female|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|             4|2005-10-01|12:50:00|  X3|       white|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|             5|2005-10-01|13:10:00|  X3|       white|     female|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|             6|2005-10-01|15:50:00|  X3|       white|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|Other Traffic Vio...|          NA|           NA|\n",
      "|             7|2005-09-11|11:45:00|  X3|       white|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|             8|2005-09-11|11:45:00|  X3|       white|     female|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|             9|2005-10-04|11:55:00|  X3|    hispanic|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|            10|2005-10-04|11:55:00|  X3|       white|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|            11|2005-10-04|14:28:00|  X3|       white|     female|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|            12|2005-10-04|14:45:00|  X3|       white|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|            13|2005-10-04|15:02:00|  X3|       white|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|            14|2005-10-04|15:35:00|  X3|       white|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|            15|2005-10-10|17:50:00|  X3|       white|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|Other Traffic Vio...|          NA|           NA|\n",
      "|            16|2005-10-10|18:10:00|  X3|       white|     female|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|Other Traffic Vio...|          NA|           NA|\n",
      "|            17|2005-10-10|18:35:00|  X3|       white|       male|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|            18|2005-10-10|19:20:00|  X3|       white|     female|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|            19|2005-10-17|11:55:00|  X3|       white|     female|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "|            20|2005-10-17|14:10:00|  X3|       white|     female|          200|vehicular|      FALSE|           TRUE|         FALSE|citation|              NA|              NA|                NA|                NA|           false|          FALSE|           FALSE|          NA|               NA|            Speeding|          NA|           NA|\n",
      "+--------------+----------+--------+----+------------+-----------+-------------+---------+-----------+---------------+--------------+--------+----------------+----------------+------------------+------------------+----------------+---------------+----------------+------------+-----------------+--------------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Intial look at the data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- raw_row_number: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- arrest_made: string (nullable = true)\n",
      " |-- citation_issued: string (nullable = true)\n",
      " |-- warning_issued: string (nullable = true)\n",
      " |-- outcome: string (nullable = true)\n",
      " |-- contraband_found: string (nullable = true)\n",
      " |-- contraband_drugs: string (nullable = true)\n",
      " |-- contraband_weapons: string (nullable = true)\n",
      " |-- contraband_alcohol: string (nullable = true)\n",
      " |-- contraband_other: boolean (nullable = true)\n",
      " |-- frisk_performed: string (nullable = true)\n",
      " |-- search_conducted: string (nullable = true)\n",
      " |-- search_basis: string (nullable = true)\n",
      " |-- reason_for_search: string (nullable = true)\n",
      " |-- reason_for_stop: string (nullable = true)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- vehicle_model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509681"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total number of values\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|subject_sex| count|\n",
      "+-----------+------+\n",
      "|         NA| 29097|\n",
      "|     female|131138|\n",
      "|       male|349446|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"subject_sex\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+\n",
      "|zone|subject_sex|count|\n",
      "+----+-----------+-----+\n",
      "|  X3|     female|26653|\n",
      "|  X3|         NA| 4627|\n",
      "|  X3|       male|62778|\n",
      "|  X4|         NA| 9679|\n",
      "|  X4|       male|94953|\n",
      "|  K3|     female|29097|\n",
      "|  X1|         NA| 3491|\n",
      "|  X1|     female| 2702|\n",
      "|  K3|       male|79771|\n",
      "|  X1|       male|10522|\n",
      "|  K1|         NA| 2252|\n",
      "|  K2|     female|28114|\n",
      "|  K1|       male|32255|\n",
      "|  NA|         NA|   10|\n",
      "|  K3|         NA| 4916|\n",
      "|  K1|     female|13855|\n",
      "|  K2|         NA| 4122|\n",
      "|  X4|     female|30717|\n",
      "|  K2|       male|69167|\n",
      "+----+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy([\"zone\", \"subject_sex\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+\n",
      "|subject_sex|     reason_for_stop| count|\n",
      "+-----------+--------------------+------+\n",
      "|     female|Other Traffic Vio...| 17911|\n",
      "|       male|Special Detail/Di...| 12977|\n",
      "|     female|Equipment/Inspect...| 14039|\n",
      "|     female|                 APB|   109|\n",
      "|     female|Violation of City...|   216|\n",
      "|         NA|    Call for Service|     4|\n",
      "|         NA|Equipment/Inspect...|     2|\n",
      "|       male|    Call for Service|  5237|\n",
      "|       male|Registration Viol...| 14181|\n",
      "|       male|            Speeding|182538|\n",
      "|         NA|            Speeding|     8|\n",
      "|       male|Motorist Assist/C...|   657|\n",
      "|       male|   Suspicious Person|   268|\n",
      "|     female|  Seatbelt Violation|  3550|\n",
      "|     female|Registration Viol...|  5649|\n",
      "|       male|Other Traffic Vio...| 72317|\n",
      "|         NA|                  NA| 29073|\n",
      "|     female|   Suspicious Person|    74|\n",
      "|         NA|  Seatbelt Violation|     3|\n",
      "|       male|                 APB|   376|\n",
      "+-----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy([\"subject_sex\", \"reason_for_stop\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the rows from which subject_sex is missing\n",
    "#Regular dropna wasn't working cuz data is a String \"NA\"\n",
    "df = df.filter(df.subject_sex.endswith('ale'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|subject_sex| count|\n",
      "+-----------+------+\n",
      "|     female|131138|\n",
      "|       male|349446|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"subject_sex\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|contraband_found| count|\n",
      "+----------------+------+\n",
      "|           FALSE| 11183|\n",
      "|              NA|462822|\n",
      "|            TRUE|  6579|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"contraband_found\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480584"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of values in data BEFORE dropping duplicates\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates from data\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480584"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of values in data AFTER dropping duplicates\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- raw_row_number: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- arrest_made: string (nullable = true)\n",
      " |-- citation_issued: string (nullable = true)\n",
      " |-- warning_issued: string (nullable = true)\n",
      " |-- outcome: string (nullable = true)\n",
      " |-- contraband_found: string (nullable = true)\n",
      " |-- contraband_drugs: string (nullable = true)\n",
      " |-- contraband_weapons: string (nullable = true)\n",
      " |-- contraband_alcohol: string (nullable = true)\n",
      " |-- contraband_other: boolean (nullable = true)\n",
      " |-- frisk_performed: string (nullable = true)\n",
      " |-- search_conducted: string (nullable = true)\n",
      " |-- search_basis: string (nullable = true)\n",
      " |-- reason_for_search: string (nullable = true)\n",
      " |-- reason_for_stop: string (nullable = true)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- vehicle_model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For checking what to drop\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Prim keys from data\n",
    "df = df.drop(\"raw_row_number\", \"department_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- arrest_made: string (nullable = true)\n",
      " |-- citation_issued: string (nullable = true)\n",
      " |-- warning_issued: string (nullable = true)\n",
      " |-- outcome: string (nullable = true)\n",
      " |-- contraband_found: string (nullable = true)\n",
      " |-- contraband_drugs: string (nullable = true)\n",
      " |-- contraband_weapons: string (nullable = true)\n",
      " |-- contraband_alcohol: string (nullable = true)\n",
      " |-- contraband_other: boolean (nullable = true)\n",
      " |-- frisk_performed: string (nullable = true)\n",
      " |-- search_conducted: string (nullable = true)\n",
      " |-- search_basis: string (nullable = true)\n",
      " |-- reason_for_search: string (nullable = true)\n",
      " |-- reason_for_stop: string (nullable = true)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- vehicle_model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     type| count|\n",
      "+---------+------+\n",
      "|vehicular|480584|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check feature type\n",
    "df.groupBy(\"type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop type feature from data, since it has only one value\n",
    "df = df.drop('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- arrest_made: string (nullable = true)\n",
      " |-- citation_issued: string (nullable = true)\n",
      " |-- warning_issued: string (nullable = true)\n",
      " |-- outcome: string (nullable = true)\n",
      " |-- contraband_found: string (nullable = true)\n",
      " |-- contraband_drugs: string (nullable = true)\n",
      " |-- contraband_weapons: string (nullable = true)\n",
      " |-- contraband_alcohol: string (nullable = true)\n",
      " |-- contraband_other: boolean (nullable = true)\n",
      " |-- frisk_performed: string (nullable = true)\n",
      " |-- search_conducted: string (nullable = true)\n",
      " |-- search_basis: string (nullable = true)\n",
      " |-- reason_for_search: string (nullable = true)\n",
      " |-- reason_for_stop: string (nullable = true)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- vehicle_model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking if type was dropped\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|        subject_race| count|\n",
      "+--------------------+------+\n",
      "|               white|344716|\n",
      "|               black| 68577|\n",
      "|            hispanic| 53123|\n",
      "|       other/unknown|  1344|\n",
      "|asian/pacific isl...| 12824|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"subject_race\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|search_conducted| count|\n",
      "+----------------+------+\n",
      "|           FALSE|462822|\n",
      "|            TRUE| 17762|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NA Values in some fields\n",
    "df.groupBy(\"search_conducted\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|contraband_found| count|\n",
      "+----------------+------+\n",
      "|           FALSE| 11183|\n",
      "|              NA|462822|\n",
      "|            TRUE|  6579|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NA Values in some fields\n",
    "df.groupBy(\"contraband_found\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "#Create a new column for contraband_found \n",
    "df = df.withColumn(\"contraband_found_resolved\", when(df.contraband_found == \"NA\", 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- arrest_made: string (nullable = true)\n",
      " |-- citation_issued: string (nullable = true)\n",
      " |-- warning_issued: string (nullable = true)\n",
      " |-- outcome: string (nullable = true)\n",
      " |-- contraband_found: string (nullable = true)\n",
      " |-- contraband_drugs: string (nullable = true)\n",
      " |-- contraband_weapons: string (nullable = true)\n",
      " |-- contraband_alcohol: string (nullable = true)\n",
      " |-- contraband_other: boolean (nullable = true)\n",
      " |-- frisk_performed: string (nullable = true)\n",
      " |-- search_conducted: string (nullable = true)\n",
      " |-- search_basis: string (nullable = true)\n",
      " |-- reason_for_search: string (nullable = true)\n",
      " |-- reason_for_stop: string (nullable = true)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- vehicle_model: string (nullable = true)\n",
      " |-- contraband_found_resolved: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+\n",
      "|contraband_found_resolved| count|\n",
      "+-------------------------+------+\n",
      "|                        1| 17762|\n",
      "|                        0|462822|\n",
      "+-------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check newly created column\n",
    "df.groupBy(\"contraband_found_resolved\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- arrest_made: string (nullable = true)\n",
      " |-- citation_issued: string (nullable = true)\n",
      " |-- warning_issued: string (nullable = true)\n",
      " |-- outcome: string (nullable = true)\n",
      " |-- contraband_found: string (nullable = true)\n",
      " |-- contraband_drugs: string (nullable = true)\n",
      " |-- contraband_weapons: string (nullable = true)\n",
      " |-- contraband_alcohol: string (nullable = true)\n",
      " |-- contraband_other: boolean (nullable = true)\n",
      " |-- frisk_performed: string (nullable = true)\n",
      " |-- search_conducted: string (nullable = true)\n",
      " |-- search_basis: string (nullable = true)\n",
      " |-- reason_for_search: string (nullable = true)\n",
      " |-- reason_for_stop: string (nullable = true)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- vehicle_model: string (nullable = true)\n",
      " |-- contraband_found_resolved: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import split function\n",
    "from pyspark.sql.functions import split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting date into different columns for LR\n",
    "from pyspark.sql.types import IntegerType\n",
    "split_date = split(df['Date'], '-')     \n",
    "df = df.withColumn('Year', split_date.getItem(0).cast(IntegerType()))\n",
    "df= df.withColumn('Month', split_date.getItem(1).cast(IntegerType()))\n",
    "df= df.withColumn('Day', split_date.getItem(2).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting time into different columns for modeling\n",
    "\n",
    "split_time = split(df['time'], '-')     \n",
    "df = df.withColumn('Hour', split_date.getItem(0).cast(IntegerType()))\n",
    "df= df.withColumn('Minute', split_date.getItem(1).cast(IntegerType()))\n",
    "df= df.withColumn('Second', split_date.getItem(2).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Second|count|\n",
      "+------+-----+\n",
      "|    31| 9308|\n",
      "|    28|15712|\n",
      "|    26|15435|\n",
      "|    27|15113|\n",
      "|    12|15510|\n",
      "|    22|15733|\n",
      "|     1|15291|\n",
      "|    13|15791|\n",
      "|    16|15617|\n",
      "|     6|16072|\n",
      "|     3|15698|\n",
      "|    20|15869|\n",
      "|     5|16197|\n",
      "|    19|15579|\n",
      "|    15|15634|\n",
      "|    17|15228|\n",
      "|     9|16208|\n",
      "|     4|16195|\n",
      "|     8|16034|\n",
      "|    23|16010|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check seconds column\n",
    "df.groupBy(\"Second\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import to_date, to_utc_timestamp\n",
    "\n",
    "\n",
    "#Creating a date time \n",
    "#df = df.select(to_date(df.date).alias('date'), 'zone', 'subject_race', 'subject_sex', 'arrest_made', 'citation_issued'\n",
    "#              , 'warning_issued', 'outcome', 'contraband_found', 'contraband_drugs', 'contraband_weapons', 'contraband_alcohol'\n",
    "#               , 'contraband_other', 'frisk_performed', 'search_conducted', 'search_basis', 'reason_for_search'\n",
    "#               , 'reason_for_stop', 'vehicle_make', 'vehicle_model', 'contraband_found_resolved')\n",
    "\n",
    "\n",
    "\n",
    "#New Discovery, dates can't be used in modeling in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- arrest_made: string (nullable = true)\n",
      " |-- citation_issued: string (nullable = true)\n",
      " |-- warning_issued: string (nullable = true)\n",
      " |-- outcome: string (nullable = true)\n",
      " |-- contraband_found: string (nullable = true)\n",
      " |-- contraband_drugs: string (nullable = true)\n",
      " |-- contraband_weapons: string (nullable = true)\n",
      " |-- contraband_alcohol: string (nullable = true)\n",
      " |-- contraband_other: boolean (nullable = true)\n",
      " |-- frisk_performed: string (nullable = true)\n",
      " |-- search_conducted: string (nullable = true)\n",
      " |-- search_basis: string (nullable = true)\n",
      " |-- reason_for_search: string (nullable = true)\n",
      " |-- reason_for_stop: string (nullable = true)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- vehicle_model: string (nullable = true)\n",
      " |-- contraband_found_resolved: integer (nullable = false)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- Second: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|2007|50139|\n",
      "|2015|45199|\n",
      "|2006|55552|\n",
      "|2013|41922|\n",
      "|2014|48878|\n",
      "|2012|57625|\n",
      "|2009|39820|\n",
      "|2005|13809|\n",
      "|2010|39440|\n",
      "|2011|42630|\n",
      "|2008|45570|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check year column\n",
    "df.groupBy(\"Year\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Month|count|\n",
      "+-----+-----+\n",
      "|   12|37648|\n",
      "|    1|43810|\n",
      "|    6|39448|\n",
      "|    3|41310|\n",
      "|    5|41375|\n",
      "|    9|39228|\n",
      "|    4|39535|\n",
      "|    8|39654|\n",
      "|    7|39896|\n",
      "|   10|40947|\n",
      "|   11|41588|\n",
      "|    2|36145|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check month column\n",
    "df.groupBy(\"Month\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Day|count|\n",
      "+---+-----+\n",
      "| 31| 9308|\n",
      "| 28|15712|\n",
      "| 26|15435|\n",
      "| 27|15113|\n",
      "| 12|15510|\n",
      "| 22|15733|\n",
      "|  1|15291|\n",
      "| 13|15791|\n",
      "| 16|15617|\n",
      "|  6|16072|\n",
      "|  3|15698|\n",
      "| 20|15869|\n",
      "|  5|16197|\n",
      "| 19|15579|\n",
      "| 15|15634|\n",
      "| 17|15228|\n",
      "|  9|16208|\n",
      "|  4|16195|\n",
      "|  8|16034|\n",
      "| 23|16010|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check day column\n",
    "df.groupBy(\"Day\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "| outcome| count|\n",
      "+--------+------+\n",
      "|      NA|  6763|\n",
      "|citation|428378|\n",
      "|  arrest| 16603|\n",
      "| warning| 28840|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check outcome column\n",
    "df.groupBy(\"outcome\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop features linearly dependent\n",
    "df = df.drop('contraband_drugs', 'contraband_weapons', 'contraband_alcohol', 'contraband_other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- arrest_made: string (nullable = true)\n",
      " |-- citation_issued: string (nullable = true)\n",
      " |-- warning_issued: string (nullable = true)\n",
      " |-- contraband_found: string (nullable = true)\n",
      " |-- frisk_performed: string (nullable = true)\n",
      " |-- search_conducted: string (nullable = true)\n",
      " |-- search_basis: string (nullable = true)\n",
      " |-- reason_for_search: string (nullable = true)\n",
      " |-- reason_for_stop: string (nullable = true)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- vehicle_model: string (nullable = true)\n",
      " |-- contraband_found_resolved: integer (nullable = false)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- Second: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop features not known prior to making the stop\n",
    "df = df.drop('arrest_made', 'frisk_performed', 'search_basis', 'reason_for_search', 'reason_for_stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop features not known prior to making the stop\n",
    "df = df.drop('warning_issued', 'citation_issued')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- contraband_found: string (nullable = true)\n",
      " |-- search_conducted: string (nullable = true)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- vehicle_model: string (nullable = true)\n",
      " |-- contraband_found_resolved: integer (nullable = false)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- Second: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|vehicle_make| count|\n",
      "+------------+------+\n",
      "|        MERK|     1|\n",
      "|          PC|     1|\n",
      "|        DODG| 14170|\n",
      "|        MASE|    30|\n",
      "|        PEUG|     1|\n",
      "|        DATS|     7|\n",
      "|        EAGL|     1|\n",
      "|        STLG|    93|\n",
      "|        FRUE|     9|\n",
      "|        LINC|  2659|\n",
      "|        INTE|     1|\n",
      "|        AMER|    15|\n",
      "|        MERZ|  3159|\n",
      "|        TRIM|     1|\n",
      "|        AMGN|    71|\n",
      "|        FIAT|    44|\n",
      "|        ISUZ|     1|\n",
      "|          NA|162525|\n",
      "|        CHRY|  7550|\n",
      "|        TRIU|    18|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check vehicle make column\n",
    "df.groupBy(\"vehicle_make\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|Distinct vehicle makes|\n",
      "+----------------------+\n",
      "|                    97|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev\n",
    "df.select(countDistinct(\"vehicle_make\").alias(\"Distinct vehicle makes\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|vehicle_model|count|\n",
      "+-------------+-----+\n",
      "|        ASTRO|   92|\n",
      "|          MDX|  590|\n",
      "|       CHR300|  123|\n",
      "|          BOX|   43|\n",
      "|           LT|    7|\n",
      "|         545I|   30|\n",
      "|      MAZDA3I|    4|\n",
      "|     6 TOURIN|    2|\n",
      "|     LE SABLE|    6|\n",
      "|    GEO PRIZM|    5|\n",
      "|          E.S|    1|\n",
      "|    TL S-TYPE|    1|\n",
      "|          ETK|    1|\n",
      "|     TEMPO GL|    4|\n",
      "|         PROS|    5|\n",
      "|      LLUMINA|    3|\n",
      "|    ENTOURAGE|   14|\n",
      "| LEGACY WAGON|    2|\n",
      "|     6 SERIES|    4|\n",
      "|          ...|   15|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check vehicle model column\n",
    "df.groupBy(\"vehicle_model\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|Distinct vehicle models|\n",
      "+-----------------------+\n",
      "|                   9184|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev\n",
    "df.select(countDistinct(\"vehicle_model\").alias(\"Distinct vehicle models\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop vehicle model\n",
    "df = df.drop('vehicle_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- contraband_found: string (nullable = true)\n",
      " |-- search_conducted: string (nullable = true)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- contraband_found_resolved: integer (nullable = false)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- Second: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|search_conducted| count|\n",
      "+----------------+------+\n",
      "|           FALSE|462822|\n",
      "|            TRUE| 17762|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NA Values in some fields\n",
    "df.groupBy(\"search_conducted\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "#Create a new column for contraband_found \n",
    "df = df.withColumn(\"search_conducted\", when(df.search_conducted == \"FALSE\", 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- contraband_found: string (nullable = true)\n",
      " |-- search_conducted: integer (nullable = false)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- contraband_found_resolved: integer (nullable = false)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- Second: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "#Create a new column for contraband_found \n",
    "df = df.withColumn(\"contraband_found\", when(df.contraband_found == \"FALSE\", 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- subject_race: string (nullable = true)\n",
      " |-- subject_sex: string (nullable = true)\n",
      " |-- contraband_found: integer (nullable = false)\n",
      " |-- search_conducted: integer (nullable = false)\n",
      " |-- vehicle_make: string (nullable = true)\n",
      " |-- contraband_found_resolved: integer (nullable = false)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- Second: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for assembler, encoder, indexer\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                OneHotEncoder,StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with categorical data\n",
    "\n",
    "# Create a string indexer (convert every string into a number, such as male = 0 and female = 1).\n",
    "# A number will be assigned to every category in the column.\n",
    "gender_indexer = StringIndexer(inputCol='subject_sex',outputCol='SexIndex')\n",
    "\n",
    "# Now we can one hot encode these numbers. This converts the various outputs into a single vector.\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndex',outputCol='SexVec')\n",
    "\n",
    "#Similar to the above.\n",
    "race_indexer = StringIndexer(inputCol='subject_race',outputCol='raceIndex')\n",
    "race_encoder = OneHotEncoder(inputCol='raceIndex',outputCol='raceVec')\n",
    "\n",
    "#Similar to the above.\n",
    "zone_indexer = StringIndexer(inputCol='zone',outputCol='zoneIndex')\n",
    "zone_encoder = OneHotEncoder(inputCol='zoneIndex',outputCol='zoneVec')\n",
    "\n",
    "#Similar to the above.\n",
    "vehicle_indexer = StringIndexer(inputCol='vehicle_make',outputCol='vehicleIndex')\n",
    "vehicle_encoder = OneHotEncoder(inputCol='vehicleIndex',outputCol='vehicleVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can assemble all of this as one vector in the features column. \n",
    "assembler = VectorAssembler(inputCols=['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second', \n",
    " 'SexVec',\n",
    " 'zoneVec',\n",
    " 'search_conducted',\n",
    " 'vehicleVec',                                      \n",
    " 'raceVec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(featuresCol='features',labelCol='contraband_found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for complex workflow\n",
    "pipeline = Pipeline(stages=[gender_indexer,race_indexer, zone_indexer, vehicle_indexer, \n",
    "                           gender_encoder,race_encoder, zone_encoder, vehicle_encoder,\n",
    "                           assembler,log_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split. \n",
    "train_data, test_data = df.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling\n",
    "fit_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data. \n",
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|contraband_found|prediction|\n",
      "+----------------+----------+\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               0|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "|               1|       1.0|\n",
      "+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the binary classifer.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='contraband_found')\n",
    "\n",
    "# If we select the actual and predicted results, we can see that some predictions were correct while others were wrong.\n",
    "results.select('contraband_found','prediction').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o438.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 4884.0 failed 1 times, most recent failure: Lost task 4.0 in stage 4884.0 (TID 336829, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: TOYO.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:266)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:128)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:223)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: TOYO.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-74ddf2149b6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We can then evaluate using AUC (area under the curve). AUC is linked to ROC.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mAUC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mAUC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o438.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 4884.0 failed 1 times, most recent failure: Lost task 4.0 in stage 4884.0 (TID 336829, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: TOYO.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:266)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:128)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:223)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: TOYO.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "# We can then evaluate using AUC (area under the curve). AUC is linked to ROC.\n",
    "AUC = my_eval.evaluate(results)\n",
    "\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
